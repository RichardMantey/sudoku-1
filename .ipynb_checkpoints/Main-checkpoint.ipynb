{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "# from Downloads import LSTMSudokuClassifier as LSTMC\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, n_label, batch_size, n_layers, use_gpu):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gpu = use_gpu\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, bidirectional=True)\n",
    "        self.hidden2label = nn.Sequential(torch.nn.Linear(2*hidden_dim, n_label),\n",
    "                                          torch.nn.Softmax(dim = 2))\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.use_gpu:\n",
    "            h0 = Variable(torch.zeros(2*n_layers, self.batch_size, self.hidden_dim).cuda())\n",
    "            c0 = Variable(torch.zeros(2*n_layers, self.batch_size, self.hidden_dim).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(2*n_layers, self.batch_size, self.hidden_dim))\n",
    "            c0 = Variable(torch.zeros(2*n_layers, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def ManipulateSquare(self, lstm_out_square):\n",
    "        counter = 0\n",
    "        inner_stack = None\n",
    "        outer_stack = None\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                x = lstm_out_square[counter,:,:]\n",
    "                counter += 1\n",
    "                x = x.view(3,3,2*hidden_dim)#change to 2*hidden_dim\n",
    "                if inner_stack is None:\n",
    "                    inner_stack = x\n",
    "                else:\n",
    "                    inner_stack = torch.cat((inner_stack,x),1)        \n",
    "            if outer_stack is None:\n",
    "#                 print(inner_stack.shape)\n",
    "                outer_stack = inner_stack\n",
    "                inner_stack = None\n",
    "#                 print('B = ',outer_stack)\n",
    "            else:\n",
    "#                 print(inner_stack.shape)\n",
    "                outer_stack = torch.cat((outer_stack,inner_stack),0)\n",
    "#                 print('B = ',outer_stack)\n",
    "                inner_stack = None\n",
    "        return outer_stack\n",
    "\n",
    "    def forward(self, row_tensor, col_tensor, square_tensor):\n",
    "#         row = row_tensor.view(self.n_features, self.batch_size, self.input_dim) #May not be necessary\n",
    "#         col = col_tensor.view(self.n_features, self.batch_size, self.input_dim) #May not be necessary\n",
    "#         square = square_tensor.view(self.n_features, self.batch_size, self.input_dim) #May not be necessary\n",
    "        lstm_out_row, hidden_row = self.lstm(row_tensor, self.hidden)\n",
    "        lstm_out_col, hidden_col = self.lstm(col_tensor, self.hidden)\n",
    "        lstm_out_square, hidden_square = self.lstm(square_tensor, self.hidden) \n",
    "        # Do manipulation Here\n",
    "        #Row & Col Manipulation\n",
    "#         print(lstm_out_row.shape, lstm_out_row)\n",
    "#         print(lstm_out_col.shape, lstm_out_col)\n",
    "        row_col = lstm_out_row.data.cpu().numpy() + np.transpose(lstm_out_col.data.cpu().numpy(), (1, 0, 2))\n",
    "#         print(\"row+col\", row_col.shape, row_col)\n",
    "        #Square Manipulation\n",
    "        final_square = self.ManipulateSquare(lstm_out_square)\n",
    "        lstm_out = row_col + final_square.data.cpu().numpy()\n",
    "#         lstm_out = lstm_out_row.data.cpu().numpy()\n",
    "#         print(\"row+col+sqaure\", lstm_out.shape, lstm_out)\n",
    "        lstm_out_tensor = Variable(torch.FloatTensor(lstm_out))\n",
    "        if self.use_gpu:\n",
    "            lstm_out_tensor = Variable(torch.FloatTensor(lstm_out).cuda())\n",
    "#         print(\"final tensor\", lstm_out_tensor.shape, lstm_out_tensor)\n",
    "        \n",
    "        #Sum Square Row Col\n",
    "        y  = self.hidden2label(lstm_out_tensor)\n",
    "#         print('Output Shape ', y.shape)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputGrid():\n",
    "    \n",
    "    def __init__(self, Grid):\n",
    "        self.Grid = Grid\n",
    "        self.gridLength = len(Grid[0])\n",
    "#         self.labels = labels\n",
    "        \n",
    "    def getlims(self, i):\n",
    "        if 0 <= i <= 2:\n",
    "            rowlims = [0,3]\n",
    "        elif 3 <= i <= 5:\n",
    "            rowlims = [3,6]\n",
    "        elif 6 <= i <= 8:\n",
    "                rowlims = [6,9]\n",
    "        return rowlims\n",
    "\n",
    "    def get_square(self, i):\n",
    "        grid_square = None\n",
    "        if i < 3:\n",
    "            if i%3 == 0:\n",
    "                grid_square = self.Grid[0:3,0:3]\n",
    "            elif i%3 == 1:\n",
    "                grid_square = self.Grid[0:3,3:6]\n",
    "            elif i%3 == 2:\n",
    "                grid_square = self.Grid[0:3,6:9]\n",
    "        elif i < 6:\n",
    "            if i%3 == 0:\n",
    "                grid_square = self.Grid[3:6,0:3]\n",
    "            elif i%3 == 1:\n",
    "                grid_square = self.Grid[3:6,3:6]\n",
    "            elif i%3 == 2:\n",
    "                grid_square = self.Grid[3:6,6:9]\n",
    "        elif i < 9:\n",
    "            if i%3 == 0:\n",
    "                grid_square = self.Grid[6:9,0:3]\n",
    "            elif i%3 == 1:\n",
    "                grid_square = self.Grid[6:9,3:6]\n",
    "            elif i%3 == 2:\n",
    "                grid_square = self.Grid[6:9,6:9]\n",
    "        return grid_square.flatten()\n",
    "                \n",
    "    \n",
    "    # takes 1D returns 2D\n",
    "    def one_hot(self, vec):\n",
    "        one_hot_matrix = []\n",
    "        for val in vec:\n",
    "            hot_vec = [0 for _ in range(9)]\n",
    "            if val > 0:\n",
    "                hot_vec[int(val)-1] = 1\n",
    "            else:\n",
    "                hot_vec = [1.0/9 for _ in range(9)]\n",
    "            one_hot_matrix.append(hot_vec)\n",
    "        real_one_hot = np.array(one_hot_matrix)\n",
    "        return real_one_hot\n",
    "    \n",
    "    #takes 2D returns 3D\n",
    "    def getInput(self):\n",
    "#         print(self.Grid)\n",
    "        Rows = np.zeros((9,9,9))\n",
    "        Columns = np.zeros((9,9,9))\n",
    "        Squares = np.zeros((9,9,9))\n",
    "        Labels = np.zeros((9,9,9))\n",
    "        for i in range(self.gridLength):\n",
    "            hot_row = self.one_hot(self.Grid[i,:])\n",
    "            hot_column = self.one_hot(self.Grid[:,i])\n",
    "            hot_square = self.one_hot(self.get_square(i))\n",
    "#             hot_label = self.one_hot(self.labels[i,:])\n",
    "            \n",
    "            Rows[i] = hot_row\n",
    "            Columns[i] = hot_column\n",
    "            Squares[i] = hot_square\n",
    "#             Labels[i] = hot_label\n",
    "        \n",
    "        row_tensor = Variable(torch.FloatTensor(Rows))\n",
    "        col_tensor = Variable(torch.FloatTensor(Columns))\n",
    "        square_tensor = Variable(torch.FloatTensor(Squares))\n",
    "#         label_tensor = Variable(torch.LongTensor(Labels))\n",
    "#         print(row_tensor)\n",
    "#         print(col_tensor)\n",
    "#         print(square_tensor)\n",
    "#         print(label_tensor)\n",
    "                 \n",
    "        return row_tensor, col_tensor, square_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_plot = True\n",
    "# use_save = True\n",
    "# if use_save:\n",
    "#     import pickle\n",
    "#     from datetime import datetime\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "TRAIN_FILE = 'sudoku_dev.csv'\n",
    "TEST_FILE = 'sudoku_test.txt'\n",
    "TRAIN_LABEL = 'train_label.txt'\n",
    "TEST_LABEL = 'test_label.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SudokuDataset(Dataset):\n",
    "    def __init__(self, fpath):\n",
    "        \n",
    "        print(fpath)\n",
    "        lines = open(fpath, 'r').read().splitlines()[1:]\n",
    "        nsamples = len(lines)\n",
    "\n",
    "        X = np.zeros((nsamples, 9*9), np.float32)  \n",
    "        Y = np.zeros((nsamples, 9*9), np.int32) \n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            quiz, solution = line.split(\",\")\n",
    "            for j, (q, s) in enumerate(zip(quiz, solution)):\n",
    "                X[i, j], Y[i, j] = q, s\n",
    "\n",
    "        X = np.reshape(X, (-1, 9, 9))\n",
    "        Y = np.reshape(Y, (-1, 9, 9))\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        quiz = self.X[index]\n",
    "        sol = self.Y[index]\n",
    "        return quiz, sol\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\sudoku_dev.csv\n",
      "[Epoch:   0/100] Training Loss: 0.027, Training Acc: 0.123\n",
      "[Epoch:   1/100] Training Loss: 0.027, Training Acc: 0.123\n",
      "[Epoch:   2/100] Training Loss: 0.027, Training Acc: 0.123\n",
      "[Epoch:   3/100] Training Loss: 0.027, Training Acc: 0.120\n",
      "[Epoch:   4/100] Training Loss: 0.027, Training Acc: 0.119\n",
      "[Epoch:   5/100] Training Loss: 0.027, Training Acc: 0.121\n",
      "[Epoch:   6/100] Training Loss: 0.027, Training Acc: 0.123\n",
      "[Epoch:   7/100] Training Loss: 0.027, Training Acc: 0.125\n",
      "[Epoch:   8/100] Training Loss: 0.027, Training Acc: 0.126\n",
      "[Epoch:   9/100] Training Loss: 0.027, Training Acc: 0.126\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a73753db6bdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquare_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m81\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m81\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-282b5590e199>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, row_tensor, col_tensor, square_tensor)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m#         col = col_tensor.view(self.n_features, self.batch_size, self.input_dim) #May not be necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m#         square = square_tensor.view(self.n_features, self.batch_size, self.input_dim) #May not be necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mlstm_out_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mlstm_out_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mlstm_out_square\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_square\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msquare_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         )\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36m_do_forward\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[0mflat_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_iter_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[0mflat_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNestedIOFunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[0mnested_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mnested_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mnested_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_map_variable_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_extended\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward_extended\u001b[1;34m(self, input, weight, hx)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0mhy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\backends\\cudnn\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(fn, input, hx, weight, output, hy)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_contiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[0mhy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    \n",
    "    ## parameter setting\n",
    "    epochs = 100\n",
    "    batch_size = 9\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    learning_rate = 1e-5\n",
    "\n",
    "    input_dim = 9\n",
    "    hidden_dim = 100\n",
    "    n_label = 9\n",
    "    n_layers = 5\n",
    "    \n",
    "    train_path = os.path.join(DATA_DIR, TRAIN_FILE)\n",
    "    test_path = os.path.join(DATA_DIR, TEST_FILE)\n",
    "\n",
    "\n",
    "     ### ********************create model**************************\n",
    "    model = LSTMClassifier(input_dim, hidden_dim, n_label, batch_size, n_layers, use_gpu)\n",
    "    if use_gpu:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    training_set = SudokuDataset(train_path)\n",
    "    train_loader = DataLoader(training_set,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0\n",
    "                          )\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    train_loss_ = []\n",
    "    test_loss_ = []\n",
    "    train_acc_ = []\n",
    "    test_acc_ = []\n",
    "    \n",
    "### training procedure\n",
    "    for epoch in range(epochs):\n",
    "        # optimizer = adjust_learning_rate(optimizer, epoch)\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        \n",
    "#         model.hidden = model.init_hidden()\n",
    "        for iter, traindata in enumerate(train_loader):\n",
    "            train_inputs_raw, train_labels_raw = traindata\n",
    "\n",
    "            train_data_numpy = Variable(train_inputs_raw).data.numpy()\n",
    "            train_labels_numpy = Variable(train_labels_raw).data.numpy()\n",
    "            train_labels = Variable(torch.LongTensor(train_labels_numpy[0]));\n",
    "    \n",
    "            input_grid = InputGrid(train_data_numpy[0])\n",
    "            row_tensor, col_tensor, square_tensor = input_grid.getInput()\n",
    "\n",
    "            if use_gpu:\n",
    "                row_tensor, col_tensor, square_tensor, train_labels = \\\n",
    "                        row_tensor.cuda(), col_tensor.cuda(), square_tensor.cuda(), train_labels.cuda()\n",
    "\n",
    "            model.hidden = model.init_hidden()\n",
    "            model.batch_size = len(train_labels)     \n",
    "            output = model(row_tensor, col_tensor, square_tensor)\n",
    "            \n",
    "            loss = loss_function(output.view(81,9), train_labels.view(81,))\n",
    "#             print(\"loss btw:\", output.view(81,9), train_labels.view(81,))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            \n",
    "\n",
    "            # calc training acc\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            predicted = predicted.add_(1)\n",
    "#             print('TrainPred =  ',predicted, 'While TrainLabel = ' ,train_labels.data)\n",
    "            total_acc += (predicted.view(81,) == train_labels.data.view(81,)).sum()\n",
    "#             print((predicted.view(81,) == train_labels.data.view(81,)).sum())\n",
    "            total += 81\n",
    "#             print(train_labels.data)\n",
    "            total_loss += loss.data[0]\n",
    "\n",
    "#        print(\"loss:\", loss.data[0], \"acc\", total_acc)\n",
    "            train_loss_.append(total_loss / total)\n",
    "            train_acc_.append(total_acc / total)\n",
    "\n",
    "        print('[Epoch: %3d/%3d] Training Loss: %.3f, Training Acc: %.3f' \n",
    "                  % (epoch, epochs, train_loss_[epoch], train_acc_[epoch]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameter setting\n",
    "epochs = 50\n",
    "batch_size = 1\n",
    "use_gpu = torch.cuda.is_available()\n",
    "learning_rate = 0.01\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
